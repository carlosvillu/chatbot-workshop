## URL: https://elpais.com/opinion/2023-05-19/el-peligro-no-es-la-inteligencia-artificial-es-openai.html ##

El peligro no es la inteligencia artificial, es OpenAI

Sam Altman embauca a los reguladores con fantasías apocalípticas para disfrazar los riesgos genuinos de su versión acelerada del capitalismo

Hay ocho millones de inteligencias en el planeta, pero el ser humano solo puede identificar dos. La primera es la suya, caracterizada por el acceso a procesos cognitivos complejos como el razonamiento, la resolución de problemas, el aprendizaje, la creatividad, la competencia emocional, la conciencia social y la adaptabilidad. La segunda es un software generativo llamado ChatGPT, cuya virtud característica es hablarnos en nuestro propio idioma. A esta inteligencia la llamamos inteligencia “artificial”. Nos interesa más que cualquier otra.

A diferencia del resto, la inteligencia artificial no puede surgir en cualquier parte. No es capaz de nacer en un establo, camino de Egipto, rodeado de bueyes y ovejas. Tampoco en lugares tan inhóspitos y variopintos como las profundidades del océano, los desiertos salinos, las regiones árticas o la barriga ácida y caliente de un volcán. Para que nazca un modelo como ChatGPT hacen falta potentes ordenadores con altos recursos de procesamiento y memoria, almacenamiento de datos escalable, entornos de desarrollo integrados, frameworks y bibliotecas de software especializados, además de conexiones de red confiables y una dieta imponente de contenidos en forma de bases de datos.

Una IA necesita espacio, mantenimiento, refrigeración y electricidad. Con este nivel de autonomía, de momento es improbable que nos sorprenda con una emboscada, o que prospere inadvertida como un virus pandémico en las favelas de Manaus o una colonia de moho en los conductos respiratorios de un hospital. Y, sin embargo, ese es el peligro que debe preocuparnos a la hora de legislar su desarrollo, al menos según el director ejecutivo de OpenAI. Sam Altman explicó el pasado martes al subcomité sobre Privacidad, Tecnología y Legislación del Senado estadounidense que su empresa planea construir y liberar sistemas cada vez más peligrosos y que necesita su ayuda para garantizar que la transición a la superinteligencia sucede sin poner a la humanidad en peligro. Propone que se regule la IA para prevenir un problema que de momento solo existe en la literatura y el cine: la Singularidad.

Los titulares del día siguiente amplifican el discurso. “El cofundador de OpenAI pide más regulación para la IA”. Sorprende que un directivo pida regulación en una industria famosa por su resistencia a ser fiscalizada. La misma semana el propio Eric Schmidt, cofundador de Google y principal asesor del Departamento de Defensa para el desarrollo de IA, aseguró en un programa de NBC News que “no hay nadie fuera de la industria capaz de entender lo que es posible. Nadie en el Gobierno capaz de hacerlo bien”. Pero existe un precedente muy cercano en el tiempo: cuando el Congreso quiso regular las criptomonedas llamaron a Sam Bankman-Fried. El fundador de FTX abrazó tan públicamente la regulación de su criptonegocio que, cuando llegó la hora de escribir las leyes, los reguladores le llamaron a él. También porque se había ganado la confianza de Washington donando públicamente millones de dólares a las campañas demócratas y en secreto una cantidad equivalente a las republicanas. Con esa estrategia, propuso diseñar a su medida la regulación de las cripto, la Ley de Protección al Consumidor de Bienes Digitales (DCCPA). Esa es la estrategia del nuevo favorito de Washington, Sam Altman, para diseñar a su medida una nueva regulación de IA.

Cabalgando el mito de la Singularidad, Altman se postulaba en el Congreso como el benévolo guardián de una nueva especie de criatura asombrosa, un unicornio salvaje capaz de llevarnos a un mundo mágico que necesita ser domado para no atravesarnos con su poderoso cuerno multicolor. “Entendemos que la gente esté ansiosa sobre cómo la IA puede cambiar nuestra forma de vida —concedió graciosamente—. Nosotros también”. El mundo mágico es un futuro de fábricas sin obreros y oficinas sin trabajadores. Un mundo sin sindicatos ni huelgas, diseñado a la medida del empresario y a conveniencia del consumidor. Si le ayudamos a domar ese unicornio podremos alcanzar el nirvana capitalista y prevenir el apocalipsis. Aunque, en el proceso, OpenAI ya haya privatizado sin permiso los contenidos de la Red, infringiendo leyes preexistentes como la propiedad intelectual y use los beneficios de ese expolio para ayudar a otras industrias a vigilar y degradar las condiciones laborales de sus trabajadores, como han entendido los guionistas del Writers Guild. Por no mencionar la crisis existencial a la que sí nos enfrentamos ahora mismo, en la realidad. Entrenar GPT-3 consume cientos de veces la energía de una vivienda y produce 502 toneladas métricas de CO₂ pero no es esa la regulación que pide Altman. ¿A quién le importan el medioambiente, la propiedad intelectual o los derechos laborales cuando nos enfrentamos a la Singularidad?

El público del Congreso fue maravillosamente receptivo. ¿Qué dos o tres reformas o regulaciones implementaría, de querer implementar alguna, si fuese reina por un día?, le preguntó el senador John Neely Kennedy. Altman quiere licencias y una agencia que las otorgue y que controle el desarrollo y uso de IA para que los modelos no licenciados puedan “autoreplicarse y autoimplantarse a lo loco”. La propuesta imita claramente al tratado de no proliferación nuclear y favorecería el monopolio de gigantes como Google, Meta, Microsoft, Anthropic y OpenAI sobre modelos abiertos y colaborativos en todo el mundo. “Estados Unidos debe liderar —dice Altman— pero para que sea efectivo, necesitamos una regulación global”.

Un informe reciente del Corporate Europe Observatory, un grupo que el poder del lobby de las grandes empresas en la UE, denunciaba la intensa campaña de presión que han emprendido estos gigantes para intervenir en la nueva ley europea de inteligencia artificial, antes de ser aprobada por los diputados del Parlamento Europeo la semana pasada. El borrador final establece que los modelos generativos como ChatGPT deberán revelar si sus modelos han sido entrenados con material con derechos de autor y los generadores de texto o imágenes, como MidJourney, tendrán que identificarse como máquinas y marcar su contenido de forma que se pueda identificar. También considera normas especiales de transparencia para los sistemas que califican de alto riesgo, como los algoritmos utilizados para gestionar a los trabajadores de una empresa o para control migratorio de fronteras por parte de un gobierno. Esos sistemas deberán cumplir requisitos de mitigación de riesgos, como mostrar los datos han utilizado para entrenar la IA y las medidas que han tomado para corregir los sesgos. Pero han eliminado el importante requisito de que los modelos sean auditados por expertos independientes y propone también la creación de un nuevo organismo de IA para establecer un centro centralizado de aplicación y control.

Un detalle importante: los modelos de IA no están protegidos como lo estaban las plataformas de responsabilizarse de los contenidos que circulan por sus servidores. No los ampara la Sección 230 ni sus equivalentes en otras partes del mundo. Tienen que garantizar que sus herramientas no produzcan contenido relacionado con abuso infantil, terrorismo, discurso de odio u cualquier otro tipo de contenido que viole la legislación de la Unión Europea. Sin embargo, ChatGPT ya produce gran parte de la propaganda que intoxica las redes sociales con el objetivo de manipular los procesos democráticos. Si Sam Altman consigue esquivar también esa clase de responsabilidades en EE UU, es improbable que le pongamos el cascabel nosotros aquí.
